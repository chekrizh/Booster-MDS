{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ковариация и корреляция.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELIIrW_vj1kR",
        "colab_type": "text"
      },
      "source": [
        "**Случайная величина** - эта некая переменная, способная принимать какие-либо значения. Результат подбрасывания монеты, температура воздуха на улице, рост случайного прохожего. Характер этих значений определеятся функцией распределения, но в сущности своей случайная величина может быть интерпретирована как некоторое измерение, в результате которого при каждой реализации случайного опыта мы получаем некоторое число.  \n",
        "Величину этого число может описать **математическое ожидание** - среднее (взвешенное по вероятностям возможных значений) значение случайной величины. \n",
        "\n",
        "Еще одним из признаков, характеризующим случайную величину - является **среднеквадратическое отклонение** (показатель рассеивания значений случайной величины относительно её математического ожидания). \n",
        "Это число, описывающие насколько сильно отличаются в среднем значения выборки от среднего значения выборки. \n",
        "\n",
        "При возведении в квадрат, эта величина представляет собой **дисперсию**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWCczu38qJmx",
        "colab_type": "text"
      },
      "source": [
        "Предположим у нас есть две случайные величины, $X$ и $Y$, каждую из которых можно описать с помощью их собственного математического ожидания и дисперсии.\n",
        "$μ_x$, $σ_x^2$ и $μ_y$, $σ_y^2$ соответсвенно.\n",
        "\n",
        "В случае если у нас возникнет вопрос о том, являются ли эти две случайные величины зависимыми, мы можем исходить из свойства математического ожидания.  \n",
        "\n",
        "Для независимых $X$ и $Y$ математическое ожидание их произведения будет равно произведению их математических ожиданий.\n",
        "\n",
        "$$ M(X \\cdot Y) = μ_x \\cdot μ_y $$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fz_-bqd0sicH",
        "colab_type": "text"
      },
      "source": [
        "А значит обратная ситуация будет свидетельстовать о наличии связи между случайными величинами. Чтобы численно описать то, как они связаны - можно посчитать отклонение между обеими частями предоплагаемого равенства. То есть насколько отличается математическое ожидание проиведения двух случайных величин от произведения их математических ожиданий.\n",
        "\n",
        "$$  cov(X, Y) = M(X \\cdot Y) - μ_x \\cdot μ_y = M[(x - μ_x) \\cdot (y - μ_y)] $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZcVqB_rxFrl",
        "colab_type": "text"
      },
      "source": [
        "Полученная величина называется **ковариацией** и ее абсолютное значние позволяет судить о линиейной зависимости. Если ковариация положительна, то с ростом значений одной случайной величины, значения второй имеют тенденцию возрастать, а если знак отрицательный — то убывать.\n",
        "\n",
        "Ковариация случайной величины с собой равна дисперсии:\n",
        "\n",
        "  $$ cov(X, X) = σ_x^2 $$\n",
        "\n",
        "Случайные величины, имеющие нулевую ковариацию, называются некоррелированными, но это еще не говорит о том, что они независмы, что является минусом, если рассматривать ковариацию с точки зрения емкости ее интрепретации.\n",
        "\n",
        "Абсолютная величина ковариации двух случайных величин не превышает среднего геометрического их дисперсий.\n",
        "\n",
        "Ковариация неудобна тем, что имеет размерность которая определяется произведением размерностей случайных величин. \n",
        "Более того, величина абсолютного значения ковариации, полученная после расчетов, говорит лишь о том как зависимы величины, но не о том как сильно. Полученное число не может быть метрикой, потому что одно и то же число может быть получено в разных условиях и зависит оно он дисперсий случайных величин.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxmTKWfT0rzQ",
        "colab_type": "text"
      },
      "source": [
        "Для того, чтобы это значение стало значимым в контексте силы зависимости случайных величин - его нужно нормировать, поделив значение ковариации на произведение среднеквадратических отклонений (квадратных корней из дисперсий).\n",
        "\n",
        "Для рассматриваемых случайных величин $X$ и $Y$ это будет выглядеть следующим образом:\n",
        "\n",
        "$$ r(X, Y) = M[\\frac{(x - μ_x)}{σ_x} \\cdot \\frac{(y - μ_y)}{σ_y}]  $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGm1bHbozA3l",
        "colab_type": "text"
      },
      "source": [
        "Это можно выразить и в упрощенном виде:\n",
        "\n",
        "$$ r(X, M) = \\frac{cov(X, M)}{σ_x \\cdot σ_y} $$\n",
        "\n",
        "Поученная величина называется **коеффициентом корреляции** двух случайных величин и является математической меры для **корреляции** - меры, характеризующей степень статистической линейной зависимости между случайными величинами.\n",
        "\n",
        "В часности это коэфициент корреляции Пирсона, который в широком примерении тождествленен корреляции в целом.\n",
        "\n",
        "Значение этого коэфициента лежит в пределах от -1 до 1, что не только является удобной и легкоинтерпретируемой формой, но и позволяет производить сравнения. Тем не менее, справедливым это значение является только в рамках конкретной выборки и не может ни быть прекцированым за ее пределы ни служить причинноследственной связью при рассмотрении факта зависимости случайных величин. А если полученное значние равно нулю - это еще не значит что связи нет, можно лишь предполагать её нелинейность."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3_2C9Q76fy9",
        "colab_type": "text"
      },
      "source": [
        "Применять **коэффициент корреляции Пирсона** можно только в случае, если наши случайные величины имеют нормальное распределение и является количественным. Стоит учитывать его неустойчивость к выбросам. При прочих равных или в случае невыполнения условности нормальности распределения сравниваемых величин - следует расчитывать корреляцию Спирмена.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvHDGAAa_tOX",
        "colab_type": "text"
      },
      "source": [
        "При расчете корреляции Спирмана, сопоставляемые показатели могут быть измерены как в непрерывной шкале, так и в порядковой.\n",
        "\n",
        "Эффективность и качество оценки методом Спирмена снижается, если разница между различными значениями какой-либо из измеряемых величин достаточно велика. Не рекомендуется использовать коэффициент Спирмена, если имеет место неравномерное распределение значений измеряемой величины.\n",
        "\n",
        "**Коэффициент корреляции Спирмена** — это мера силы монотонной взаимосвязи между двумя случайными величинами. Для того чтобы ее посчитать,  нужно взять  $X$ и $Y$, превратить значение в каждой из подвыборок в ранги и уже на этих рангах посчитать значение коэффициента корреляции Пирсона. Именно за счет вот этого вот рангового преобразования мы получаем, что корреляция Спирмена устойчива к любой монотонной взаимосвязи между $X$ и $Y$, поскольку ранговое преобразование превращает любую монотонную взаимосвязь в линейную. \n",
        "\n",
        "Если обозначить разность рангов для каждой пары значений случайных величин как d, то формулу для расчета корреляции Спирмана можно представить как:\n",
        " $$ p = 1 - \\frac{6 \\sum d_i^2}{n \\cdot (n^2 - 1)}  $$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Корреляция Спирмена наследует часть свойств корреляции Пирсона.\n",
        "\n",
        "Вроде того, что она примиает значения от -1 до 1, где 0 говорит о отсутствии взаимосвязи. Более того, если распределение все-таки нормальное, то  коэффициент корреляции Спирмена может быть использован для оценки коэффициента корреляции Пирсона r по формуле:\n",
        "\n",
        "$$ r = 2 \\cdot sin{\\frac{\\pi}{6}}p\\$$\n",
        "\n",
        "Статистическая значимость полученного коэффициента оценивается при помощи t-критерия Стьюдента. Если расчитанное значение t-критерия меньше табличного при заданном числе степеней свободы, статистическая значимость наблюдаемой взаимосвязи - отсутствует. Если больше, то корреляционная связь считается статистически значимой.  \n",
        "Недостатком коэффициента корреляции рангов является то, что одинаковым разностям рангов могут соответствовать совершенно отличные разности значений (в случае количественных признаков). Недоучет размеров отклонений признаков от их средних величин занижает меру тесноты связи. Поэтому для количественных признаков корреляция рангов обладает меньшей информативностью, чем коэффициент корреляции числовых значений этих признаков."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3shwFCjVEKj1",
        "colab_type": "text"
      },
      "source": [
        "Но это все хорошо, если нужно считать корреляцию для непрерывных величин, иначе нужны другие подходы.\n",
        "\n",
        "Например, для бинарных величин - корреляция Мэтьюса, для категориальных - коэффициент Крамера (этот уже принимает значения от 0 до 1)\n",
        "\n",
        "Выбор подхода для определения связи между величинами и сам процесс определения называется **корреляционным анализом**, сам же коефициент корреляции по мимо статистической значимости и информационной ценности имеет применение и в качестве составной части более сложных процессов, к примеру при растчете линейно регрессии, в часности для расчета ее параметров."
      ]
    }
  ]
}